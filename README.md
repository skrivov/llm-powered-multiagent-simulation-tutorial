# Introduction to Agent-Based Simulation with LLMs
Agent-based simulations are a powerful method for modeling complex systems, where individual entities, known as agents, interact with one another and their environment according to predefined rules. The integration of Large Language Models (LLMs) like OpenAI's GPT-4 enables these agents to exhibit sophisticated, human-like behaviors and natural language understanding. By leveraging LLMs, we can create simulations that not only react dynamically to various inputs but also generate rich and contextually appropriate responses, making them ideal for scenarios ranging from entertainment to educational and training applications.

## Installing Dependencies

To run the tutorial, you'll need to install a few dependencies. These include the openai package, which allows you to interact with the OpenAI API, and python-dotenv, which helps manage environment variables securely. You can install these dependencies using pip by running the following command:

```
pip install openai python-dotenv
```
Ensure these packages are installed before running the tutorial code to avoid any issues.

## Obtaining an OpenAI API Key
To access the capabilities of OpenAI's language models, you'll need an API key, which acts as a secure token for authenticating your requests. First, sign up for an OpenAI account at OpenAI's website. Once registered, navigate to the API section of your dashboard, where you can create a new API key. Be sure to store this key securely, as it grants access to your OpenAI account and usage limits. For safe practice, avoid hardcoding the key directly in your code; instead, use environment variables or secure storage mechanisms to keep your credentials safe. The best practice is to store API keys in a file *.env* This file should never be commited to piblic version controlsystem, such as GitHub. The API key is used for an initialization of OpenAI client as follows:

```python
# The .env file should contain the API key in the format: OPENAI_API_KEY=your_openai_api_key
# This function looks for a file named .env in the current directory and loads the contents into the environment variables
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
```

## Understanding System Prompts

A system prompt is a foundational concept when working with LLMs in agent-based simulations. It sets the stage for the agent's behavior by providing a specific context or persona that the model should assume. For instance, if you're simulating a famous comedian, the system prompt might include details about their style of humor and notable traits. This initial instruction guides the LLM's responses, ensuring consistency and relevance throughout the simulation. The system prompt is crucial as it encapsulates the essence of the agent, dictating how it interacts within the simulated environment.

Here's an example of initializing a system prompt for an agent:

```python
class Agent:
    def __init__(self, name, role):
        """
        Initialize an Agent with a name and role.
        
        :param name: The name of the character (e.g., "Groucho Marx")
        :param role: The role or notable characteristic (e.g., "known for quick wit and one-liner jokes")
        """
        self.name = name
        self.role = role
        self.system_prompt = f"You are {self.name}, a famous comedian known for {self.role}."

# Example of creating an agent
groucho = Agent("Groucho Marx", "his quick wit and one-liner jokes")

```
The system prompt is used to set the context for the agent's behavior before it generates a response. This is done by including the system prompt in the messages parameter when calling the OpenAI API. Here’s how it works inside function act():

```python
def act(self, context):
        """
        Generate a response from the agent based on the given context.

        :param context: The context or prompt for the agent (e.g., "Tell a one-liner joke.")
        :return: The generated response from the agent
        """
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": context}
            ],
            ......
        )
        return response.choices[0].message.content
```
The system prompt (self.system_prompt) is sent as the first message with the role system, which primes the model to adopt the persona and style defined by the agent. This ensures that the subsequent response generated by the model is consistent with the character's identity, making the interaction more authentic and aligned with the agent's role.


## Implementing the Simulation Loop
The core of the agent-based simulation is the simulation loop, which iteratively processes each agent's actions. In this program, the loop takes a list of agents, each defined by a unique system prompt, and a context that they respond to—such as "Tell a one-liner joke." For each round of the simulation, the loop prompts the agents to generate responses based on their system prompts and the shared context. The outputs are then printed, demonstrating the dynamic interactions between agents and the environment.

```python
def simulation_loop(agents, rounds):
    # Define a shared context for the agents to respond to
    context = "Tell a one-liner joke."
    
    # Loop through the specified number of rounds
    for _ in range(rounds):
        # Iterate over each agent in the list
        for agent in agents:
            # Generate a response using the agent's system prompt and the shared context
            completion = agent.act(context)
            # Print the agent's name and their generated response
            print(f"{agent.name}: {completion.strip()}")

if __name__ == "__main__":
    # Initialize a list of agents, each representing a famous comedian
    agents = [
        Agent("Groucho Marx", "his quick wit and one-liner jokes"),
        Agent("George Carlin", "his observational humor and clever wordplay"),
        Agent("Rodney Dangerfield", "his self-deprecating humor and catchphrase 'I don't get no respect'")
    ]
    
    # Run the simulation loop with the defined agents for a specified number of rounds
    simulation_loop(agents, 2)

```
We have created a simple program with three agent. Here is the complete [code](sequential-comedians.py)

The output from the program looks like this:
```
Groucho Marx: "I find television very educational. Every time someone turns it on, I go in the other room and read a book."
George Carlin: "Ever noticed that anyone driving slower than you is an idiot, but anyone going faster is a maniac?"
Rodney Dangerfield: I told my psychiatrist that everyone hates me. He said I was being ridiculous - everyone hasn't met me yet!
....

Execution time: 11.08 seconds
```

## Boosting Simulation Speed with Asynchronous Programming
Asynchronous programming can significantly enhance the performance of agent-based simulations. For example, by switching from a sequential setup to using OpenAI's async API, we reduced the execution time from 11.08 seconds to just 4.31 seconds. Here's how:

Using the AsyncOpenAI client, we can handle multiple API requests at once, allowing for non-blocking code execution. The key elements include the async and await keywords, which enable concurrent task execution. For instance, instead of waiting for each agent's response sequentially, we can use asyncio.gather() to collect all responses simultaneously.


```python
# Initialize the async client
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
```

```python
# Define an async function for agent actions
async def act(self, context):
    response = await client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": context}
        ]
    )
    return response.choices[0].message.content
```

The use of `await` here allows the program to run other tasks concurrently while waiting for the API to return a response. By using asyncio.gather(), multiple agents can execute their actions at the same time, ensuring that all operations are handled efficiently and the results are collected once all tasks are complete.

```python
# Run multiple agents concurrently
async def simulation_loop(agents, rounds):
    tasks = [agent.act(context) for agent in agents]
    completions = await asyncio.gather(*tasks)
    for agent, completion in zip(agents, completions):
        print(f"{agent.name}: {completion.strip()}")
```

By adopting asynchronous programming, we speed up the simulation and enhance scalability, making it possible to efficiently handle more agents and rounds. Here is a complet code for the [asyncronious version of the program](asynchronious-comedians.py).

Now that we know the basic code patterns of an LLM based multiagent simulator, we can write more entertaining programs. In the next version of the program, we  add a jury agent to evaluate the comedians' performances. The new addition here is the information flow from the comedians to the jury agent, where the jury receives and analyzes all the jokes before making a decision. You can explore this program [here](comedians-and-jury.py).

## Conversation of Agents with Memory

In this section, we explore how agents in a simulation can maintain their own memory of the conversation, allowing for more dynamic and individualized interactions. This approach gives each agent a personalized view of the dialogue, enabling them to generate responses that are contextually relevant based on their memory. The key elements of the program are as follows:


**Agent's Memory Initialization**

   Each agent is initialized with a `messages` list that starts with a system prompt defining their persona. This list is used to store all interactions the agent is part of.

   ```python
   self.messages = [{"role": "system", "content": system_prompt}]
   ```

**Generating Responses and Updating Memory** 
   
The `act` method within each agent serves two crucial functions: generating a response based on the conversation history and updating the agent's memory with this new response. This combined process allows the agent to maintain a continuous and contextually aware dialogue. Here’s how it works:

```python
def act(self):
    # Generate a response using the agent's current conversation history
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=self.messages
    )

    # Clean up the response content
    response_content = response.choices[0].message.content.strip()

    ....
    # Update the agent's memory by appending the new response
    self.messages.append({"role": "assistant", "content": response_content})

    return response_content
```
**Sharing Information Among Agents** 

After an agent generates a response, it’s important that this response is shared with all other agents so they can incorporate it into their own conversation history. This process ensures that each agent is aware of the entire dialogue, making the conversation more coherent and interactive. Here’s how information is shared among agents:

```python
for other_agent in agents:
    if other_agent != agent:
        # Append the current agent's response to the other agents' conversation histories
        other_agent.messages.append({"role": "assistant", "content": f"{agent.name}: {response}"})
```
This code iterates over all agents in the simulation. For each agent that did not generate the current response, the response is added to their messages list. This way, every agent stays updated with the latest interactions, allowing them to respond more appropriately in subsequent rounds.  

The whole program can be found [here](conversation.py). Here is a sample output from this program.

RRound 1:

Ronald Reagan: Well, you know, they say politics is the second oldest profession, but it bears a striking resemblance to the first!

Richard Nixon: And sometimes it feels like we spent more time dodging scandals than actually running the country!

Jimmy Carter: Well, at least we had peanuts to fall back on. The only scandals on the farm were when the tractor broke down!

Round 2:

Ronald Reagan: Yes, Jimmy, you had peanuts, but some days I felt like we were just working for peanuts!

Richard Nixon: I've always said, Jimmy, if you want a friend in Washington, get a dog. But even my dog didn't trust the press!

Jimmy Carter: Well, I might not have had scandals like some, but my rabbits sure did make headlines!
Execution time: 4.60 seconds

## Presidential Debate Program: Bringing Political Discourse to Life

The presidential debate program offers a compelling use case for agent-based simulations. It brings to life a debate between Donald Trump and Kamala Harris, with each candidate responding in line with their distinct personality traits. Users can observe how different audience groups—Liberal Democrats, Conservatives, and Independents—react to the debate.

This program highlights the dynamic nature of political discourse, showing how AI-driven agents can simulate real-world interactions with context, memory, and unique personas. By engaging with this simulation, you'll gain valuable insights into how AI models complex conversations, making it both an educational and engaging experience.


You should now be familiar with all the code patterns used in this program. Here is [the code for the presidential debate program.](presidential_debates.py)

Below is a fragment of a sample output from the program, showcasing how the agent system prompts capture the unique styles of the debate participants. By carefully fine-tuning these prompts, you can enhance the realism of the simulation. Adjusting the prompts allows you to significantly alter the participants' styles, response lengths, and overall behavior. Exploring these adjustments is left as an exercise for the readers.

 Remember, the system prompt is crucial in providing identity to the agent and is the essence of agent-based simulation. Mastering the art of crafting system prompts is crucial. The power to shape an agent's personality, style, and behavior lies in the details of these prompts. As you experiment with these adjustments, you hold the key to unlocking the power of LLM-driven intelligent agents —an opportunity to create truly immersive and realistic simulations. This challenge is now yours to explore.

### Part of Output

Round 1:

Bret Baier: How do you propose to address the rising concerns about climate change while ensuring economic growth and job creation?

Bret Baier: Kamala Harris, you are the first to answer.

Kamala Harris: We can tackle climate change and spur economic growth together by investing in clean energy technologies, which create millions of good-paying jobs. By transitioning to renewable energy, improving infrastructure, and incentivizing green innovation, we protect our environment and ensure a resilient, forward-looking economy for all Americans.

Bret Baier: Donald Trump, your response.

Donald Trump: Bret, let me tell you, nobody wants clean air and clean water more than Donald Trump, okay? We need smart solutions, not those radical regulations that kill jobs and destroy businesses. We can unleash American energy—clean, beautiful energy—while protecting our environment. Oil, gas, solar, wind—we can do it all and make it work for the American people. Keep our economy strong, bring back jobs, and make America energy-independent. That's the Trump plan.

Bret Baier: Kamala Harris, your rebuttal.

Kamala Harris: Donald, we need a comprehensive and forward-thinking approach to true energy independence. Investing heavily in renewable energy sources like solar and wind, alongside responsible oversight, will create sustainable jobs without jeopardizing our environment. It's not about rolling back regulations; it's about smart, innovative policies that protect our planet and ensure long-term economic growth for all Americans.

Bret Baier: Donald Trump, your rebuttal.

Donald Trump: Kamala, I appreciate your passion, but let's be real—solar and wind aren't enough to power this great nation alone. We need a balanced approach. Under my administration, we achieved energy independence by boosting all energy sectors, not just renewables. Regulations should protect without strangling our economy. We can innovate and grow without killing jobs or hiking everyone's energy bills. That’s the Trump way—common sense, not radical change.

Bret Baier: Kamala Harris, your rebuttal.

Kamala Harris: Donald, the climate crisis demands urgent action beyond the status quo. Relying heavily on fossil fuels isn't sustainable. We need a balanced, but forward-focused energy policy that prioritizes clean, renewable sources to protect our future. Innovation in green energy is not radical; it's necessary for economic resilience, global competitiveness, and the well-being of future generations. Let's lead with vision, not just reaction.

Bret Baier: Donald Trump, your rebuttal.

Donald Trump: Kamala, you're missing the point. We can’t just flip a switch and go 100% renewable overnight. America needs practical solutions that work now. I’m all for innovation, but not at the cost of our economy and jobs. We were energy-independent under my presidency because of an all-of-the-above strategy. Let's invest wisely in renewables while making sure our economy thrives. Leadership means making smart decisions, not just green slogans. That’s real vision.

